{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Top 30 Repositories for each Topic on Github\n",
    "\n",
    "![Github Image](https://preview.redd.it/g38817mqb1361.png?width=960&crop=smart&auto=webp&s=063b72aee824ef4f41cd58b3944b877d8a7f23e8)\n",
    "\n",
    "\n",
    "\n",
    "*Author: [Lafir](https://www.linkedin.com/in/lafir)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: \n",
    "\n",
    "In this project, we will scrape top 30 repositories for each topic on github page using `requests` and `BeautifulSoup` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Outline:\n",
    "\n",
    "- We have to scrape this paginated webpage: https://github.com/topics, to get the entire list of topics available on Github\n",
    "- Use the `requests` library to download the page and `BeautifulSoup` library to parse and extract the information\n",
    "- Create a dataframe for the list of Topics which should contain topic title, topic page URL, and topic description\n",
    "\n",
    "- With the dataframe created, use the `requests` library to download the page for each topic using the topic page URL\n",
    "- Now use the `BeautifulSoup` library to parse and extract the information from the downloaded topic pages\n",
    "- Create a dataframe for each topic which should contain username, repo name, stars, and repo URL of top 30 repositories and it should be in the below format:\n",
    "\n",
    "```\n",
    "username,repo_name,stars,repo_url\n",
    "mrdoob,three.js,83700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,20200,https://github.com/libgdx/libgdx\n",
    "```\n",
    "\n",
    "#### Note:\n",
    "1. To create a CSV file, prepare a dictionary of lists\n",
    "2. Create a pandas dataframe from dictionary of lists using `pd.DataFrame()` method\n",
    "3. Convert the dataframe into CSV file using `df.to_csv()` method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the required libraries\n",
    "!pip install requests beautifulsoup4 pandas jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to download webpages\n",
    "import requests\n",
    "\n",
    "#to parse and extract information from downloaded pages\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#to work with OS and files \n",
    "import os\n",
    "\n",
    "#to create dataframes\n",
    "import pandas as pd\n",
    "\n",
    "#to save our notebook online\n",
    "import jovian\n",
    "\n",
    "#to prevent warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the list of Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Create Soup Doc\n",
    "\n",
    "Let's define a helper function to download the topics list web pages which spans over 6 pages using `requests` library and parse the downloaded information using `BeautifulSoup` class from `bs4` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soupify_topics_list_pages():\n",
    "    \"\"\"\"\n",
    "    It Scrapes the entire list of topics which spans over 6 web pages.\n",
    "    Returns a beautifulsoup doc.\n",
    "    \"\"\"\n",
    "    all_page_contents = ''\n",
    "    topics_list_page_url = 'https://github.com/topics?page='\n",
    "    for i in range(1,7):\n",
    "        response = requests.get(topics_list_page_url+str(i))\n",
    "        #check successful response\n",
    "        if response.status_code != 200:\n",
    "            raise Exception('Failed to load page{}'.format(topic_url))\n",
    "        single_page_contents = response.text\n",
    "        all_page_contents += single_page_contents\n",
    "    #parse using beautiful soup\n",
    "    doc = BeautifulSoup(all_page_contents, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Topic Titles\n",
    "\n",
    "Once the soup doc was created using the `soupify_topics_list_pages()`, we have to go through the webpage html to extract the required information like topic title, topic description and topic URL using the inspect elemets option which is available in browsers like `Google Chrome`, `Brave`, `Mozilla Firefox` etc.\n",
    "\n",
    "![Topic Title Class Image](https://i.imgur.com/F8DWSTB.png)\n",
    "\n",
    "`topic_title_class` was identified as `'f3 lh-condensed mb-0 mt-1 Link--primary'` using the inspect elements option available under developer tools.\n",
    "\n",
    "Using the `topic_title_class`, we collected all the `<p> tags` which contains the topic title and the topic title was extracted using `text()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    \"\"\"\"\n",
    "    It accepts the soup doc as an input and returns topic titles list.\n",
    "    \"\"\"\n",
    "    topic_title_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': topic_title_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text.strip())  #it's always safer to apply strip() fn. while extracting strings\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Topic Descriptions\n",
    "\n",
    "Similar to the process of extracting topic titles, topic descriptions were extracted after identifying `'f5 color-fg-muted mb-0 mt-1'` as `topic_desc_class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    \"\"\"\"\n",
    "    It accepts the soup doc as an input and returns topic descriptions list.\n",
    "    \"\"\"\n",
    "    topic_desc_class = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class': topic_desc_class})\n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Topic URLs\n",
    "\n",
    "URL for each topics were created by concatenating the base URL `\"https://github.com\"` with text available in `<a> tag` of class name `'no-underline flex-1 d-flex flex-column'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    \"\"\"\"\n",
    "    It accepts the soup doc as an input and returns topic URLs list.\n",
    "    \"\"\"\n",
    "    topic_url_class = 'no-underline flex-1 d-flex flex-column'\n",
    "    topic_url_tags = doc.find_all('a', {'class': topic_url_class})\n",
    "    topic_urls = []\n",
    "    base_url = \"https://github.com\"\n",
    "    for tag in topic_url_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topics List Dataframe\n",
    "\n",
    "Now we have 3 functions, each of which returns a list of topic title, topic description and topic URL. \n",
    "\n",
    "Let's define another helper function where these lists are gathered to create a dictionary which will be later converted into pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics_list_df(doc):\n",
    "    \"\"\"\n",
    "    Returns a dataframe that consists of Topic name, Topic Description and Topic URL.\n",
    "    \"\"\"\n",
    "    topics_dict = {'title': get_topic_titles(doc),\n",
    "            'description': get_topic_descs(doc),\n",
    "            'url': get_topic_urls(doc)    \n",
    "        }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping top repos for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once topics list were scraped successfully, we have to scrape top 30 repositories (each topic page contains only 30 repos) for each topic page.\n",
    "\n",
    "1. Download each topic page using topic URL from topic_df\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soupify_topics_page(topic_url):\n",
    "    \"\"\"\n",
    "    It downloads each topics page using topic URL.\n",
    "    Returns a soup doc using BeautifulSoup.   \n",
    "    \"\"\"\n",
    "    # download the page\n",
    "    response = requests.get(topic_url)\n",
    "    #check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page{}'.format(topic_url))\n",
    "    #parse using beautiful soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_repos_df(topic_doc):\n",
    "    #get h3 tags containing reponame, username, repo url\n",
    "    h3_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': h3_class})\n",
    "    span_id = 'repo-stars-counter-star'\n",
    "    star_tags = topic_doc.find_all('span', {'id': span_id})\n",
    "    \n",
    "    topic_repos_dict = {\n",
    "        'username': [],\n",
    "        'repo_name': [],\n",
    "        'stars': [],\n",
    "        'repo_url': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(repo_tag, star_tag):\n",
    "    #returns all the required info about a repository\n",
    "    a_tags = repo_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic_page_and_create_csv():\n",
    "    \"\"\" Scrape top repositories of each topic.\n",
    "    \"\"\"\n",
    "    print('Scraping Topics List')\n",
    "    topics_df = (create_topics_list_df(soupify_topics_list_pages()))\n",
    "    \n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repos for \"{}\"'.format(row['title']))\n",
    "        create_top_repos_csv(row['url'], 'data/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_repos_csv(topic_url, fpath):\n",
    "    if os.path.exists(fpath):\n",
    "        print('The file {} already exists. Skipping...'.format(fpath))\n",
    "        return\n",
    "    top_repos_df = create_top_repos_df(soupify_topics_page(topic_url))\n",
    "    top_repos_df.to_csv(fpath, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Topics List\n",
      "Scraping top repos for \"3D\"\n",
      "The file data/3D.csv already exists. Skipping...\n",
      "Scraping top repos for \"Ajax\"\n",
      "The file data/Ajax.csv already exists. Skipping...\n",
      "Scraping top repos for \"Algorithm\"\n",
      "The file data/Algorithm.csv already exists. Skipping...\n",
      "Scraping top repos for \"Amp\"\n",
      "The file data/Amp.csv already exists. Skipping...\n",
      "Scraping top repos for \"Android\"\n",
      "The file data/Android.csv already exists. Skipping...\n",
      "Scraping top repos for \"Angular\"\n",
      "The file data/Angular.csv already exists. Skipping...\n",
      "Scraping top repos for \"Ansible\"\n",
      "The file data/Ansible.csv already exists. Skipping...\n",
      "Scraping top repos for \"API\"\n",
      "The file data/API.csv already exists. Skipping...\n",
      "Scraping top repos for \"Arduino\"\n",
      "The file data/Arduino.csv already exists. Skipping...\n",
      "Scraping top repos for \"ASP.NET\"\n",
      "The file data/ASP.NET.csv already exists. Skipping...\n",
      "Scraping top repos for \"Atom\"\n",
      "The file data/Atom.csv already exists. Skipping...\n",
      "Scraping top repos for \"Awesome Lists\"\n",
      "The file data/Awesome Lists.csv already exists. Skipping...\n",
      "Scraping top repos for \"Amazon Web Services\"\n",
      "The file data/Amazon Web Services.csv already exists. Skipping...\n",
      "Scraping top repos for \"Azure\"\n",
      "The file data/Azure.csv already exists. Skipping...\n",
      "Scraping top repos for \"Babel\"\n",
      "The file data/Babel.csv already exists. Skipping...\n",
      "Scraping top repos for \"Bash\"\n",
      "The file data/Bash.csv already exists. Skipping...\n",
      "Scraping top repos for \"Bitcoin\"\n",
      "The file data/Bitcoin.csv already exists. Skipping...\n",
      "Scraping top repos for \"Bootstrap\"\n",
      "The file data/Bootstrap.csv already exists. Skipping...\n",
      "Scraping top repos for \"Bot\"\n",
      "The file data/Bot.csv already exists. Skipping...\n",
      "Scraping top repos for \"C\"\n",
      "The file data/C.csv already exists. Skipping...\n",
      "Scraping top repos for \"Chrome\"\n",
      "The file data/Chrome.csv already exists. Skipping...\n",
      "Scraping top repos for \"Chrome extension\"\n",
      "The file data/Chrome extension.csv already exists. Skipping...\n",
      "Scraping top repos for \"Command line interface\"\n",
      "The file data/Command line interface.csv already exists. Skipping...\n",
      "Scraping top repos for \"Clojure\"\n",
      "The file data/Clojure.csv already exists. Skipping...\n",
      "Scraping top repos for \"Code quality\"\n",
      "The file data/Code quality.csv already exists. Skipping...\n",
      "Scraping top repos for \"Code review\"\n",
      "The file data/Code review.csv already exists. Skipping...\n",
      "Scraping top repos for \"Compiler\"\n",
      "The file data/Compiler.csv already exists. Skipping...\n",
      "Scraping top repos for \"Continuous integration\"\n",
      "The file data/Continuous integration.csv already exists. Skipping...\n",
      "Scraping top repos for \"COVID-19\"\n",
      "The file data/COVID-19.csv already exists. Skipping...\n",
      "Scraping top repos for \"C++\"\n",
      "The file data/C++.csv already exists. Skipping...\n",
      "Scraping top repos for \"Cryptocurrency\"\n",
      "The file data/Cryptocurrency.csv already exists. Skipping...\n",
      "Scraping top repos for \"Crystal\"\n",
      "The file data/Crystal.csv already exists. Skipping...\n",
      "Scraping top repos for \"C#\"\n",
      "The file data/C#.csv already exists. Skipping...\n",
      "Scraping top repos for \"CSS\"\n",
      "The file data/CSS.csv already exists. Skipping...\n",
      "Scraping top repos for \"Data structures\"\n",
      "The file data/Data structures.csv already exists. Skipping...\n",
      "Scraping top repos for \"Data visualization\"\n",
      "The file data/Data visualization.csv already exists. Skipping...\n",
      "Scraping top repos for \"Database\"\n",
      "The file data/Database.csv already exists. Skipping...\n",
      "Scraping top repos for \"Deep learning\"\n",
      "The file data/Deep learning.csv already exists. Skipping...\n",
      "Scraping top repos for \"Dependency management\"\n",
      "The file data/Dependency management.csv already exists. Skipping...\n",
      "Scraping top repos for \"Deployment\"\n",
      "The file data/Deployment.csv already exists. Skipping...\n",
      "Scraping top repos for \"Django\"\n",
      "The file data/Django.csv already exists. Skipping...\n",
      "Scraping top repos for \"Docker\"\n",
      "The file data/Docker.csv already exists. Skipping...\n",
      "Scraping top repos for \"Documentation\"\n",
      "The file data/Documentation.csv already exists. Skipping...\n",
      "Scraping top repos for \".NET\"\n",
      "The file data/.NET.csv already exists. Skipping...\n",
      "Scraping top repos for \"Electron\"\n",
      "The file data/Electron.csv already exists. Skipping...\n",
      "Scraping top repos for \"Elixir\"\n",
      "The file data/Elixir.csv already exists. Skipping...\n",
      "Scraping top repos for \"Emacs\"\n",
      "The file data/Emacs.csv already exists. Skipping...\n",
      "Scraping top repos for \"Ember\"\n",
      "The file data/Ember.csv already exists. Skipping...\n",
      "Scraping top repos for \"Emoji\"\n",
      "The file data/Emoji.csv already exists. Skipping...\n",
      "Scraping top repos for \"Emulator\"\n",
      "The file data/Emulator.csv already exists. Skipping...\n",
      "Scraping top repos for \"ESLint\"\n",
      "The file data/ESLint.csv already exists. Skipping...\n",
      "Scraping top repos for \"Ethereum\"\n",
      "The file data/Ethereum.csv already exists. Skipping...\n",
      "Scraping top repos for \"Express\"\n",
      "The file data/Express.csv already exists. Skipping...\n",
      "Scraping top repos for \"Firebase\"\n",
      "The file data/Firebase.csv already exists. Skipping...\n",
      "Scraping top repos for \"Firefox\"\n",
      "The file data/Firefox.csv already exists. Skipping...\n",
      "Scraping top repos for \"Flask\"\n",
      "The file data/Flask.csv already exists. Skipping...\n",
      "Scraping top repos for \"Font\"\n",
      "The file data/Font.csv already exists. Skipping...\n",
      "Scraping top repos for \"Framework\"\n",
      "The file data/Framework.csv already exists. Skipping...\n",
      "Scraping top repos for \"Front end\"\n",
      "The file data/Front end.csv already exists. Skipping...\n",
      "Scraping top repos for \"Game engine\"\n",
      "The file data/Game engine.csv already exists. Skipping...\n",
      "Scraping top repos for \"Git\"\n",
      "The file data/Git.csv already exists. Skipping...\n",
      "Scraping top repos for \"GitHub API\"\n",
      "The file data/GitHub API.csv already exists. Skipping...\n",
      "Scraping top repos for \"Go\"\n",
      "The file data/Go.csv already exists. Skipping...\n",
      "Scraping top repos for \"Google\"\n",
      "The file data/Google.csv already exists. Skipping...\n",
      "Scraping top repos for \"Gradle\"\n",
      "The file data/Gradle.csv already exists. Skipping...\n",
      "Scraping top repos for \"GraphQL\"\n",
      "The file data/GraphQL.csv already exists. Skipping...\n",
      "Scraping top repos for \"Gulp\"\n",
      "The file data/Gulp.csv already exists. Skipping...\n",
      "Scraping top repos for \"Hacktoberfest\"\n",
      "The file data/Hacktoberfest.csv already exists. Skipping...\n",
      "Scraping top repos for \"Haskell\"\n",
      "The file data/Haskell.csv already exists. Skipping...\n",
      "Scraping top repos for \"Homebrew\"\n",
      "The file data/Homebrew.csv already exists. Skipping...\n",
      "Scraping top repos for \"Homebridge\"\n",
      "The file data/Homebridge.csv already exists. Skipping...\n",
      "Scraping top repos for \"HTML\"\n",
      "The file data/HTML.csv already exists. Skipping...\n",
      "Scraping top repos for \"HTTP\"\n",
      "The file data/HTTP.csv already exists. Skipping...\n",
      "Scraping top repos for \"Icon font\"\n",
      "The file data/Icon font.csv already exists. Skipping...\n",
      "Scraping top repos for \"iOS\"\n",
      "The file data/iOS.csv already exists. Skipping...\n",
      "Scraping top repos for \"IPFS\"\n",
      "The file data/IPFS.csv already exists. Skipping...\n",
      "Scraping top repos for \"Java\"\n",
      "The file data/Java.csv already exists. Skipping...\n",
      "Scraping top repos for \"JavaScript\"\n",
      "The file data/JavaScript.csv already exists. Skipping...\n",
      "Scraping top repos for \"Jekyll\"\n",
      "The file data/Jekyll.csv already exists. Skipping...\n",
      "Scraping top repos for \"jQuery\"\n",
      "The file data/jQuery.csv already exists. Skipping...\n",
      "Scraping top repos for \"JSON\"\n",
      "The file data/JSON.csv already exists. Skipping...\n",
      "Scraping top repos for \"The Julia Language\"\n",
      "The file data/The Julia Language.csv already exists. Skipping...\n",
      "Scraping top repos for \"Jupyter Notebook\"\n",
      "The file data/Jupyter Notebook.csv already exists. Skipping...\n",
      "Scraping top repos for \"Koa\"\n",
      "The file data/Koa.csv already exists. Skipping...\n",
      "Scraping top repos for \"Kotlin\"\n",
      "The file data/Kotlin.csv already exists. Skipping...\n",
      "Scraping top repos for \"Kubernetes\"\n",
      "The file data/Kubernetes.csv already exists. Skipping...\n",
      "Scraping top repos for \"Laravel\"\n",
      "The file data/Laravel.csv already exists. Skipping...\n",
      "Scraping top repos for \"LaTeX\"\n",
      "The file data/LaTeX.csv already exists. Skipping...\n",
      "Scraping top repos for \"Library\"\n",
      "The file data/Library.csv already exists. Skipping...\n",
      "Scraping top repos for \"Linux\"\n",
      "The file data/Linux.csv already exists. Skipping...\n",
      "Scraping top repos for \"Localization\"\n",
      "The file data/Localization.csv already exists. Skipping...\n",
      "Scraping top repos for \"Lua\"\n",
      "The file data/Lua.csv already exists. Skipping...\n",
      "Scraping top repos for \"Machine learning\"\n",
      "The file data/Machine learning.csv already exists. Skipping...\n",
      "Scraping top repos for \"macOS\"\n",
      "The file data/macOS.csv already exists. Skipping...\n",
      "Scraping top repos for \"Markdown\"\n",
      "The file data/Markdown.csv already exists. Skipping...\n",
      "Scraping top repos for \"Mastodon\"\n",
      "The file data/Mastodon.csv already exists. Skipping...\n",
      "Scraping top repos for \"Material design\"\n",
      "The file data/Material design.csv already exists. Skipping...\n",
      "Scraping top repos for \"MATLAB\"\n",
      "The file data/MATLAB.csv already exists. Skipping...\n",
      "Scraping top repos for \"Maven\"\n",
      "The file data/Maven.csv already exists. Skipping...\n",
      "Scraping top repos for \"Minecraft\"\n",
      "The file data/Minecraft.csv already exists. Skipping...\n",
      "Scraping top repos for \"Mobile\"\n",
      "The file data/Mobile.csv already exists. Skipping...\n",
      "Scraping top repos for \"Monero\"\n",
      "The file data/Monero.csv already exists. Skipping...\n",
      "Scraping top repos for \"MongoDB\"\n",
      "The file data/MongoDB.csv already exists. Skipping...\n",
      "Scraping top repos for \"Mongoose\"\n",
      "The file data/Mongoose.csv already exists. Skipping...\n",
      "Scraping top repos for \"Monitoring\"\n",
      "The file data/Monitoring.csv already exists. Skipping...\n",
      "Scraping top repos for \"MvvmCross\"\n",
      "The file data/MvvmCross.csv already exists. Skipping...\n",
      "Scraping top repos for \"MySQL\"\n",
      "The file data/MySQL.csv already exists. Skipping...\n",
      "Scraping top repos for \"NativeScript\"\n",
      "The file data/NativeScript.csv already exists. Skipping...\n",
      "Scraping top repos for \"Nim\"\n",
      "The file data/Nim.csv already exists. Skipping...\n",
      "Scraping top repos for \"Natural language processing\"\n",
      "The file data/Natural language processing.csv already exists. Skipping...\n",
      "Scraping top repos for \"Node.js\"\n",
      "The file data/Node.js.csv already exists. Skipping...\n",
      "Scraping top repos for \"NoSQL\"\n",
      "The file data/NoSQL.csv already exists. Skipping...\n",
      "Scraping top repos for \"npm\"\n",
      "The file data/npm.csv already exists. Skipping...\n",
      "Scraping top repos for \"Objective-C\"\n",
      "The file data/Objective-C.csv already exists. Skipping...\n",
      "Scraping top repos for \"OpenGL\"\n",
      "The file data/OpenGL.csv already exists. Skipping...\n",
      "Scraping top repos for \"Operating system\"\n",
      "The file data/Operating system.csv already exists. Skipping...\n",
      "Scraping top repos for \"P2P\"\n",
      "The file data/P2P.csv already exists. Skipping...\n",
      "Scraping top repos for \"Package manager\"\n",
      "The file data/Package manager.csv already exists. Skipping...\n",
      "Scraping top repos for \"Parsing\"\n",
      "The file data/Parsing.csv already exists. Skipping...\n",
      "Scraping top repos for \"Perl\"\n",
      "The file data/Perl.csv already exists. Skipping...\n",
      "Scraping top repos for \"Phaser\"\n",
      "The file data/Phaser.csv already exists. Skipping...\n",
      "Scraping top repos for \"PHP\"\n",
      "The file data/PHP.csv already exists. Skipping...\n",
      "Scraping top repos for \"PICO-8\"\n",
      "The file data/PICO-8.csv already exists. Skipping...\n",
      "Scraping top repos for \"Pixel Art\"\n",
      "The file data/Pixel Art.csv already exists. Skipping...\n",
      "Scraping top repos for \"PostgreSQL\"\n",
      "The file data/PostgreSQL.csv already exists. Skipping...\n",
      "Scraping top repos for \"Project management\"\n",
      "The file data/Project management.csv already exists. Skipping...\n",
      "Scraping top repos for \"Publishing\"\n",
      "The file data/Publishing.csv already exists. Skipping...\n",
      "Scraping top repos for \"PWA\"\n",
      "The file data/PWA.csv already exists. Skipping...\n",
      "Scraping top repos for \"Python\"\n",
      "The file data/Python.csv already exists. Skipping...\n",
      "Scraping top repos for \"Qt\"\n",
      "The file data/Qt.csv already exists. Skipping...\n",
      "Scraping top repos for \"R\"\n",
      "The file data/R.csv already exists. Skipping...\n",
      "Scraping top repos for \"Rails\"\n",
      "The file data/Rails.csv already exists. Skipping...\n",
      "Scraping top repos for \"Raspberry Pi\"\n",
      "The file data/Raspberry Pi.csv already exists. Skipping...\n",
      "Scraping top repos for \"Ratchet\"\n",
      "The file data/Ratchet.csv already exists. Skipping...\n",
      "Scraping top repos for \"React\"\n",
      "The file data/React.csv already exists. Skipping...\n",
      "Scraping top repos for \"React Native\"\n",
      "The file data/React Native.csv already exists. Skipping...\n",
      "Scraping top repos for \"ReactiveUI\"\n",
      "The file data/ReactiveUI.csv already exists. Skipping...\n",
      "Scraping top repos for \"Redux\"\n",
      "The file data/Redux.csv already exists. Skipping...\n",
      "Scraping top repos for \"REST API\"\n",
      "The file data/REST API.csv already exists. Skipping...\n",
      "Scraping top repos for \"Ruby\"\n",
      "The file data/Ruby.csv already exists. Skipping...\n",
      "Scraping top repos for \"Rust\"\n",
      "The file data/Rust.csv already exists. Skipping...\n",
      "Scraping top repos for \"Sass\"\n",
      "The file data/Sass.csv already exists. Skipping...\n",
      "Scraping top repos for \"Scala\"\n",
      "The file data/Scala.csv already exists. Skipping...\n",
      "Scraping top repos for \"scikit-learn\"\n",
      "The file data/scikit-learn.csv already exists. Skipping...\n",
      "Scraping top repos for \"Software-defined networking\"\n",
      "The file data/Software-defined networking.csv already exists. Skipping...\n",
      "Scraping top repos for \"Security\"\n",
      "The file data/Security.csv already exists. Skipping...\n",
      "Scraping top repos for \"Server\"\n",
      "The file data/Server.csv already exists. Skipping...\n",
      "Scraping top repos for \"Serverless\"\n",
      "The file data/Serverless.csv already exists. Skipping...\n",
      "Scraping top repos for \"Shell\"\n",
      "The file data/Shell.csv already exists. Skipping...\n",
      "Scraping top repos for \"Sketch\"\n",
      "The file data/Sketch.csv already exists. Skipping...\n",
      "Scraping top repos for \"SpaceVim\"\n",
      "The file data/SpaceVim.csv already exists. Skipping...\n",
      "Scraping top repos for \"Spring Boot\"\n",
      "The file data/Spring Boot.csv already exists. Skipping...\n",
      "Scraping top repos for \"SQL\"\n",
      "The file data/SQL.csv already exists. Skipping...\n",
      "Scraping top repos for \"Storybook\"\n",
      "The file data/Storybook.csv already exists. Skipping...\n",
      "Scraping top repos for \"Support\"\n",
      "The file data/Support.csv already exists. Skipping...\n",
      "Scraping top repos for \"Swift\"\n",
      "The file data/Swift.csv already exists. Skipping...\n",
      "Scraping top repos for \"Symfony\"\n",
      "The file data/Symfony.csv already exists. Skipping...\n",
      "Scraping top repos for \"Telegram\"\n",
      "The file data/Telegram.csv already exists. Skipping...\n",
      "Scraping top repos for \"Tensorflow\"\n",
      "The file data/Tensorflow.csv already exists. Skipping...\n",
      "Scraping top repos for \"Terminal\"\n",
      "The file data/Terminal.csv already exists. Skipping...\n",
      "Scraping top repos for \"Terraform\"\n",
      "The file data/Terraform.csv already exists. Skipping...\n",
      "Scraping top repos for \"Testing\"\n",
      "The file data/Testing.csv already exists. Skipping...\n",
      "Scraping top repos for \"Twitter\"\n",
      "The file data/Twitter.csv already exists. Skipping...\n",
      "Scraping top repos for \"TypeScript\"\n",
      "The file data/TypeScript.csv already exists. Skipping...\n",
      "Scraping top repos for \"Ubuntu\"\n",
      "The file data/Ubuntu.csv already exists. Skipping...\n",
      "Scraping top repos for \"Unity\"\n",
      "The file data/Unity.csv already exists. Skipping...\n",
      "Scraping top repos for \"Unreal Engine\"\n",
      "The file data/Unreal Engine.csv already exists. Skipping...\n",
      "Scraping top repos for \"Vagrant\"\n",
      "The file data/Vagrant.csv already exists. Skipping...\n",
      "Scraping top repos for \"Vim\"\n",
      "The file data/Vim.csv already exists. Skipping...\n",
      "Scraping top repos for \"Virtual reality\"\n",
      "The file data/Virtual reality.csv already exists. Skipping...\n",
      "Scraping top repos for \"Vue.js\"\n",
      "The file data/Vue.js.csv already exists. Skipping...\n",
      "Scraping top repos for \"Wagtail\"\n",
      "The file data/Wagtail.csv already exists. Skipping...\n",
      "Scraping top repos for \"Web Components\"\n",
      "The file data/Web Components.csv already exists. Skipping...\n",
      "Scraping top repos for \"Web app\"\n",
      "The file data/Web app.csv already exists. Skipping...\n",
      "Scraping top repos for \"Webpack\"\n",
      "The file data/Webpack.csv already exists. Skipping...\n",
      "Scraping top repos for \"Windows\"\n",
      "The file data/Windows.csv already exists. Skipping...\n",
      "Scraping top repos for \"WordPlate\"\n",
      "The file data/WordPlate.csv already exists. Skipping...\n",
      "Scraping top repos for \"WordPress\"\n",
      "The file data/WordPress.csv already exists. Skipping...\n",
      "Scraping top repos for \"Xamarin\"\n",
      "The file data/Xamarin.csv already exists. Skipping...\n",
      "Scraping top repos for \"XML\"\n",
      "The file data/XML.csv already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "scrape_topic_page_and_create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"lafirm/github-top-repo-scraping\" on https://jovian.ai/\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/lafirm/github-top-repo-scraping\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/lafirm/github-top-repo-scraping'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit(project='github-top-repos-scraping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
